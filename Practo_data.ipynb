{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/POOJACH76/Procto/blob/main/Practo_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k273xFJwa2mo",
        "outputId": "f6ac3886-a06d-4c4b-f32e-5023b2688816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.16.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.23.2-py3-none-any.whl (461 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.6/461.6 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.16.0 trio-0.23.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8b17EgF5mU5",
        "outputId": "b409b0f9-5458-4578-e561-8ed54bf5a641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping and data compilation completed for Tripura districts.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of Tripura districts\n",
        "tripura_districts = [\n",
        "    \"north-tripura\", \"south-tripura\", \"west-tripura\", \"dhalai\", \"sipahijala\", \"unakoti\", \"gomati\", \"khowai\"\n",
        "]\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Create an empty DataFrame to store the scraped data\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for district in tripura_districts:\n",
        "    # Initialize WebDriver for each district\n",
        "    wd = webdriver.Chrome(options=options)\n",
        "    url = f\"https://www.practo.com/{district}/doctors\"\n",
        "    wd.get(url)\n",
        "    time.sleep(3)  # Allow 3 seconds for the webpage to load\n",
        "\n",
        "    scroll_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "    new_height = 0\n",
        "\n",
        "    while True:\n",
        "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight-1500);\")  # Scroll to the last page\n",
        "        time.sleep(3)  # Scroll time\n",
        "\n",
        "        new_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "        if new_height == scroll_height:\n",
        "            break  # Break the loop if the previous page height is the same as the new height after scrolling and refreshing\n",
        "        scroll_height = new_height  # Assigning\n",
        "\n",
        "    # Scrape data for the current district\n",
        "    urls = []\n",
        "    link = []\n",
        "    doc_name = []\n",
        "    exp = []\n",
        "    info = []\n",
        "    fee = []\n",
        "    types = []\n",
        "    city = []\n",
        "    local = []\n",
        "    rating = []\n",
        "    feedback = []\n",
        "\n",
        "    soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "    for parent in soup.find_all(\"div\", class_=\"u-border-general--bottom\"):\n",
        "        anchor = parent.find('a')\n",
        "        if anchor:\n",
        "            second_half = anchor.get('href')\n",
        "            link.append(\"https://www.practo.com/\" + second_half)\n",
        "        else:\n",
        "            link.append(None)\n",
        "\n",
        "        doctor_name_element = parent.select(\".doctor-name\")\n",
        "        if doctor_name_element:\n",
        "            doc_name.append(doctor_name_element[0].get_text())\n",
        "        else:\n",
        "            doc_name.append(\"N/A\")\n",
        "        exp_element = parent.select(\".uv2-spacer--xs-top\")\n",
        "        if exp_element:\n",
        "            exp_text = exp_element[0].get_text().split()\n",
        "            if exp_text:\n",
        "                exp.append(exp_text[0])\n",
        "            else:\n",
        "                exp.append(\"N/A\")\n",
        "        else:\n",
        "            exp.append(\"N/A\")\n",
        "\n",
        "        info_element = parent.find(\"div\", class_=\"info-section\")\n",
        "        if info_element:\n",
        "            info.append(info_element.text)\n",
        "        else:\n",
        "            info.append(\"N/A\")\n",
        "\n",
        "        fee_element = parent.find('span', attrs={'data-qa-id': 'consultation_fee'})\n",
        "        if fee_element:\n",
        "            fee.append(fee_element.text)\n",
        "        else:\n",
        "            fee.append(\"N/A\")\n",
        "\n",
        "        locality_element = parent.find('span', attrs={'data-qa-id': 'practice_locality'})\n",
        "        if locality_element:\n",
        "            local.append(locality_element.text)\n",
        "        else:\n",
        "            local.append(\"N/A\")\n",
        "\n",
        "        city_element = parent.find('span', attrs={'data-qa-id': 'practice_city'})\n",
        "        if city_element:\n",
        "            city.append(city_element.text)\n",
        "        else:\n",
        "            city.append(\"N/A\")\n",
        "\n",
        "        types_element = parent.find(\"div\", class_=\"u-grey_3-text\")\n",
        "        if types_element:\n",
        "            types_div = types_element.find(\"div\", class_=\"u-d-flex\")\n",
        "            if types_div:\n",
        "                types_span = types_div.span\n",
        "                if types_span:\n",
        "                    types.append(types_span.text)\n",
        "                else:\n",
        "                    types.append(\"N/A\")\n",
        "            else:\n",
        "                types.append(\"N/A\")\n",
        "        else:\n",
        "            types.append(\"N/A\")\n",
        "\n",
        "        rating_element = parent.find('span', attrs={'data-qa-id': 'doctor_recommendation'})\n",
        "        if rating_element:\n",
        "            rating.append(rating_element.text)\n",
        "        else:\n",
        "            rating.append(\"N/A\")\n",
        "\n",
        "        feedback_element = parent.find('span', attrs={'data-qa-id': 'total_feedback'})\n",
        "        if feedback_element:\n",
        "            feedback.append(feedback_element.text)\n",
        "        else:\n",
        "            feedback.append(\"N/A\")\n",
        "\n",
        "\n",
        "    wd.quit()\n",
        "\n",
        "\n",
        "    district_data = {\n",
        "        \"District\": [district] * len(link),\n",
        "        \"Doctor Name\": doc_name,\n",
        "        \"Doctor Profile Link\": link,\n",
        "        \"Experience\": exp,\n",
        "        \"Info\": info,\n",
        "        \"Consultation Fee\": fee,\n",
        "        \"Locality\": local,\n",
        "        \"City\": city,\n",
        "        \"Types\": types,\n",
        "        \"Rating\": rating,\n",
        "        \"Feedback\": feedback\n",
        "    }\n",
        "\n",
        "    district_df = pd.DataFrame(district_data)\n",
        "\n",
        "\n",
        "    all_data = pd.concat([all_data, district_df], ignore_index=True)\n",
        "\n",
        "\n",
        "all_data.to_csv(\"doctors_data_tripura.csv\", index=False)\n",
        "\n",
        "print(\"Scraping and data compilation completed for Tripura districts.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa3G0ns7a3jE",
        "outputId": "93309284-9dc3-4ca8-9234-ed76344711db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to doctors_data.csv\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Open it, go to a website, and get results\n",
        "wd = webdriver.Chrome(options=options)\n",
        "wd.get(\"https://www.practo.com/Bokaro/doctors\")\n",
        "time.sleep(3)  # Allow 3 seconds for the web page to open\n",
        "\n",
        "scroll_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "new_height = 0\n",
        "\n",
        "while True:\n",
        "    wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight-1500);\")  # Scroll to the last page\n",
        "    time.sleep(3)  # Scroll time\n",
        "\n",
        "    new_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "    if new_height == scroll_height:\n",
        "        break  # Breaks the loop if the previous page height is the same as the new height after scrolling and refreshing\n",
        "    scroll_height = new_height  # Assigning\n",
        "\n",
        "urls = []\n",
        "link = []\n",
        "doc_name = []\n",
        "exp = []\n",
        "info = []\n",
        "fee = []\n",
        "types = []\n",
        "city = []\n",
        "local = []\n",
        "rating = []\n",
        "feedback = []\n",
        "\n",
        "soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "\n",
        "for parent in soup.find_all(\"div\", class_=\"u-border-general--bottom\"):\n",
        "    anchor = parent.find('a')\n",
        "    if anchor:\n",
        "        second_half = anchor.get('href')\n",
        "        link.append(\"https://www.practo.com/\" + second_half)  # Links of all doctors\n",
        "    else:\n",
        "        link.append(None)  # Or any other suitable placeholder if a link is not found\n",
        "\n",
        "    doctor_name_element = parent.select(\".doctor-name\")\n",
        "    if doctor_name_element:\n",
        "        doc_name.append(doctor_name_element[0].get_text())  # Doctor name\n",
        "    else:\n",
        "        doc_name.append(\"N/A\")  # Placeholder for missing doctor name\n",
        "\n",
        "    exp_element = parent.select(\".uv2-spacer--xs-top\")\n",
        "    if exp_element:\n",
        "        exp.append(exp_element[0].get_text().split()[0])  # Doctor experience\n",
        "    else:\n",
        "        exp.append(\"N/A\")  # Placeholder for missing experience\n",
        "\n",
        "    info_element = parent.find(\"div\", class_=\"info-section\")\n",
        "    if info_element:\n",
        "        info.append(info_element.text)\n",
        "    else:\n",
        "        info.append(\"N/A\")  # Placeholder for missing info\n",
        "\n",
        "    fee_element = parent.find('span', attrs={'data-qa-id': 'consultation_fee'})\n",
        "    if fee_element:\n",
        "        fee.append(fee_element.text)\n",
        "    else:\n",
        "        fee.append(\"N/A\")  # Placeholder for missing fee\n",
        "\n",
        "    locality_element = parent.find('span', attrs={'data-qa-id': 'practice_locality'})\n",
        "    if locality_element:\n",
        "        local.append(locality_element.text)\n",
        "    else:\n",
        "        local.append(\"N/A\")  # Placeholder for missing locality\n",
        "\n",
        "    city_element = parent.find('span', attrs={'data-qa-id': 'practice_city'})\n",
        "    if city_element:\n",
        "        city.append(city_element.text)\n",
        "    else:\n",
        "        city.append(\"N/A\")  # Placeholder for missing city\n",
        "\n",
        "    types_element = parent.find(\"div\", class_=\"u-grey_3-text\")\n",
        "    if types_element:\n",
        "        types_div = types_element.find(\"div\", class_=\"u-d-flex\")\n",
        "        if types_div:\n",
        "            types_span = types_div.span\n",
        "            if types_span:\n",
        "                types.append(types_span.text)\n",
        "            else:\n",
        "                types.append(\"N/A\")\n",
        "        else:\n",
        "            types.append(\"N/A\")\n",
        "    else:\n",
        "        types.append(\"N/A\")\n",
        "\n",
        "    rating_element = parent.find('span', attrs={'data-qa-id': 'doctor_recommendation'})\n",
        "    if rating_element:\n",
        "        rating.append(rating_element.text)\n",
        "    else:\n",
        "        rating.append(\"N/A\")  # Placeholder for missing rating\n",
        "\n",
        "    feedback_element = parent.find('span', attrs={'data-qa-id': 'total_feedback'})\n",
        "    if feedback_element:\n",
        "        feedback.append(feedback_element.text)\n",
        "    else:\n",
        "        feedback.append(\"N/A\")\n",
        "\n",
        "# Create a DataFrame from the scraped data\n",
        "data = {\n",
        "    \"Doctor Name\": doc_name,\n",
        "    \"Doctor Profile Link\": link,\n",
        "    \"Experience\": exp,\n",
        "    \"Info\": info,\n",
        "    \"Consultation Fee\": fee,\n",
        "    \"Locality\": local,\n",
        "    \"City\": city,\n",
        "    \"Types\": types,\n",
        "    \"Rating\": rating,\n",
        "    \"Feedback\": feedback\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(\"doctors_data.csv\", index=False)\n",
        "\n",
        "# Print a message to confirm that the data has been saved\n",
        "print(\"Data has been saved to doctors_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG61eoSAOyrH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YOfAt1fKUfT",
        "outputId": "821a7595-4ba1-4511-dee8-3bcfb25d9e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping and data compilation completed.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# List of districts in Tamil Nadu\n",
        "districts = [\n",
        "    \"Anantapur\", \"Chittoor\", \"East Godavari\", \"Guntur\", \"Krishna\",\n",
        "    \"Kurnool\", \"Nellore\", \"Prakasam\", \"Srikakulam\", \"Visakhapatnam\",\n",
        "    \"Vizianagaram\", \"West Godavari\", \"Y.S.R. Kadapa\"\n",
        "]\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Create an empty DataFrame to store the scraped data\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "# Initialize WebDriver\n",
        "wd = webdriver.Chrome(options=options)\n",
        "\n",
        "for district in districts:\n",
        "    url = f\"https://www.practo.com/{district}/doctors\"  # Replace with the actual URL for doctors in the specific district\n",
        "    wd.get(url)\n",
        "    time.sleep(3)  # Allow 3 seconds for the webpage to load\n",
        "\n",
        "    scroll_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "    new_height = 0\n",
        "\n",
        "    while True:\n",
        "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight-1500);\")  # Scroll to the last page\n",
        "        time.sleep(3)  # Scroll time\n",
        "\n",
        "        new_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "        if new_height == scroll_height:\n",
        "            break  # Break the loop if the previous page height is the same as the new height after scrolling and refreshing\n",
        "        scroll_height = new_height  # Assigning\n",
        "\n",
        "    # Scrape data for the current district\n",
        "    urls = []\n",
        "    link = []\n",
        "    doc_name = []\n",
        "    exp = []\n",
        "    info = []\n",
        "    fee = []\n",
        "    types = []\n",
        "    locality = []\n",
        "    rating = []\n",
        "    feedback = []\n",
        "\n",
        "    soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "    for parent in soup.find_all(\"div\", class_=\"c-card\"):\n",
        "        anchor = parent.find('a', class_='c-card__link')\n",
        "        if anchor:\n",
        "            link.append(\"https://www.practo.com\" + anchor.get('href'))  # Links of all doctors\n",
        "            doc_name_element = anchor.find('h2', class_='c-card__doctor-name')\n",
        "            if doc_name_element:\n",
        "                doc_name.append(doc_name_element.text.strip())  # Doctor name\n",
        "            else:\n",
        "                doc_name.append(\"N/A\")  # Placeholder for missing doctor name\n",
        "\n",
        "            exp_element = anchor.find('div', class_='c-card__experience')\n",
        "            if exp_element:\n",
        "                exp.append(exp_element.text.strip())  # Doctor experience\n",
        "            else:\n",
        "                exp.append(\"N/A\")  # Placeholder for missing experience\n",
        "\n",
        "            info_element = anchor.find('div', class_='c-card__info')\n",
        "            if info_element:\n",
        "                info.append(info_element.text.strip())  # Info\n",
        "            else:\n",
        "                info.append(\"N/A\")  # Placeholder for missing info\n",
        "\n",
        "            fee_element = anchor.find('span', class_='c-card__consultation-fee')\n",
        "            if fee_element:\n",
        "                fee.append(fee_element.text.strip())  # Consultation Fee\n",
        "            else:\n",
        "                fee.append(\"N/A\")  # Placeholder for missing fee\n",
        "\n",
        "            locality_element = anchor.find('span', class_='c-card__neighborhood')\n",
        "            if locality_element:\n",
        "                locality.append(locality_element.text.strip())  # Locality\n",
        "            else:\n",
        "                locality.append(\"N/A\")  # Placeholder for missing locality\n",
        "\n",
        "            types_element = anchor.find('div', class_='c-card__tags')\n",
        "            if types_element:\n",
        "                types.append(types_element.text.strip())  # Types\n",
        "            else:\n",
        "                types.append(\"N/A\")  # Placeholder for missing types\n",
        "\n",
        "            rating_element = anchor.find('span', class_='c-star-rating__text')\n",
        "            if rating_element:\n",
        "                rating.append(rating_element.text.strip())  # Rating\n",
        "            else:\n",
        "                rating.append(\"N/A\")  # Placeholder for missing rating\n",
        "\n",
        "            feedback_element = anchor.find('span', class_='c-card__feedback')\n",
        "            if feedback_element:\n",
        "                feedback.append(feedback_element.text.strip())  # Feedback\n",
        "            else:\n",
        "                feedback.append(\"N/A\")  # Placeholder for missing feedback\n",
        "\n",
        "    # Create a DataFrame for the current district's data\n",
        "    district_data = {\n",
        "        \"District\": [district] * len(link),\n",
        "        \"Doctor Name\": doc_name,\n",
        "        \"Doctor Profile Link\": link,\n",
        "        \"Experience\": exp,\n",
        "        \"Info\": info,\n",
        "        \"Consultation Fee\": fee,\n",
        "        \"Locality\": locality,\n",
        "        \"Types\": types,\n",
        "        \"Rating\": rating,\n",
        "        \"Feedback\": feedback\n",
        "    }\n",
        "\n",
        "    district_df = pd.DataFrame(district_data)\n",
        "\n",
        "    # Append the current district's data to the overall DataFrame\n",
        "    all_data = pd.concat([all_data, district_df], ignore_index=True)\n",
        "\n",
        "# Save the overall DataFrame to a CSV file\n",
        "all_data.to_csv(\"doctors_data_tamilnadu.csv\", index=False)\n",
        "\n",
        "# Close the WebDriver\n",
        "wd.quit()\n",
        "\n",
        "print(\"Scraping and data compilation completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWBr3-V4Paiy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjwXIcuMuZ1o4+N7incTj3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}